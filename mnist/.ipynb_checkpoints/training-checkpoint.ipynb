{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('mnist/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform binary classification on the dataset instead of multi-class classification as a test\n",
    "#as such, only records with a label of 0 or 1 are used\n",
    "\n",
    "#filtering\n",
    "binary_df = training_data[training_data['label'].isin([0,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binary_df.drop(columns=['label']).values\n",
    "y = binary_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7052, 784)\n",
      "(7052,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twoLayerNet():\n",
    "    def __init__(self) -> None:\n",
    "        self.params = {}\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "    \n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        z1 = np.einsum(\"ij,jk->ik\",W1.T,X) + b1\n",
    "        a1 = self.sigmoid(z1)\n",
    "        z2 = np.einsum(\"ij,jk->k\",W2.T,a1) + b2\n",
    "        a2 = self.sigmoid(z2)\n",
    "        \n",
    "        return z1, a1, z2, a2\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y, learning_rate, z1, a1, z2, a2):\n",
    "        \n",
    "        n0 = X.shape[0]\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        dz2 = a2 - y\n",
    "        dW2 = (1/m) * np.dot(dz2, a1.T)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=1, keepdims=True)\n",
    "        dz1 = np.dot(self.params['W2'], dz2) * (a1 * (1 - a1))\n",
    "        dW1 = np.dot(dz1, X.T)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)\n",
    "        \n",
    "        \n",
    "        self.params['W1'] -= learning_rate * dW1.T\n",
    "        self.params['W2'] -= learning_rate * dW2.T\n",
    "        self.params['b1'] -= learning_rate * db1\n",
    "        self.params['b2'] -= learning_rate * db2 \n",
    "        \n",
    "        \n",
    "    def loss(self,y_pred, y, m):\n",
    "        y_pred = y_pred.ravel()\n",
    "        return (1/m) * sum(-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        n0 = X.shape[0]\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        #normalization\n",
    "        X = 1/X.max() * X\n",
    "        \n",
    "        self.params['W1'] = np.random.randn(n0, 4)\n",
    "        self.params['b1'] = np.random.randn(4,1)\n",
    "        self.params['W2'] = np.random.randn(4,1)\n",
    "        self.params['b2'] = np.random.randn(1,1)\n",
    "        \n",
    "        z1, a1, z2, a2 = self.forward(X)\n",
    "        \n",
    "        while self.loss(a2, y, m) > 0.01:\n",
    "            print(\"loss: \", self.loss(a2, y, m))\n",
    "            self.backward(X, y, 0.01, z1, a1, z2, a2)\n",
    "            z1, a1, z2, a2 = self.forward(X)\n",
    "            \n",
    "        return self.params\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = 1/X.max() * X\n",
    "        z1, a1, z2, a2 = self.forward(X)\n",
    "        return np.where(a2 > 0.5, 1, 0).ravel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.8089477471029587\n",
      "loss:  0.3660634891446625\n",
      "loss:  0.3349869905378522\n",
      "loss:  0.3263086988593461\n",
      "loss:  0.3219721337319788\n",
      "loss:  0.3192265718275978\n",
      "loss:  0.3171328885013937\n",
      "loss:  0.31542298779636174\n",
      "loss:  0.3139536964815555\n",
      "loss:  0.31262984865110804\n",
      "loss:  0.3113989242477312\n",
      "loss:  0.3102389787271168\n",
      "loss:  0.3091418380753673\n",
      "loss:  0.3081010146360993\n",
      "loss:  0.30710931924399165\n",
      "loss:  0.30615892308807774\n",
      "loss:  0.30524125498438864\n",
      "loss:  0.30434826722938796\n",
      "loss:  0.30347334886781613\n",
      "loss:  0.30261199493750646\n",
      "loss:  0.30176314889357686\n",
      "loss:  0.30092981785836886\n",
      "loss:  0.300115987815994\n",
      "loss:  0.2993221374193032\n",
      "loss:  0.29854533540918915\n",
      "loss:  0.2977821192010881\n",
      "loss:  0.29702989491683324\n",
      "loss:  0.2962869578872414\n",
      "loss:  0.2955522094391109\n",
      "loss:  0.29482492685511197\n",
      "loss:  0.294104613916612\n",
      "loss:  0.2933909056447848\n",
      "loss:  0.2926835074774205\n",
      "loss:  0.2919821584363376\n",
      "loss:  0.2912866118642283\n",
      "loss:  0.2905966281434352\n",
      "loss:  0.28991197425650683\n",
      "loss:  0.28923242619560424\n",
      "loss:  0.28855777175582104\n",
      "loss:  0.2878878125666574\n",
      "loss:  0.2872223650659236\n",
      "loss:  0.2865612605437368\n",
      "loss:  0.285904344528087\n",
      "loss:  0.28525147577874427\n",
      "loss:  0.28460252509143\n",
      "loss:  0.28395737404069427\n",
      "loss:  0.2833159137347147\n",
      "loss:  0.2826780436256024\n",
      "loss:  0.2820436704094671\n",
      "loss:  0.2814127070491225\n",
      "loss:  0.28078507194755936\n",
      "loss:  0.28016068828724533\n",
      "loss:  0.2795394835301292\n",
      "loss:  0.2789213890530091\n",
      "loss:  0.27830633987895054\n",
      "loss:  0.2776942744616167\n",
      "loss:  0.2770851344853845\n",
      "loss:  0.27647886465653165\n",
      "loss:  0.27587541247552555\n",
      "loss:  0.27527472799350144\n",
      "loss:  0.27467676356513326\n",
      "loss:  0.2740814736144143\n",
      "loss:  0.27348881442966494\n",
      "loss:  0.2728987440006916\n",
      "loss:  0.2723112219055253\n",
      "loss:  0.2717262092481102\n",
      "loss:  0.27114366864219963\n",
      "loss:  0.2705635642309208\n",
      "loss:  0.2699858617258546\n",
      "loss:  0.2694105284437952\n",
      "loss:  0.26883753331331656\n",
      "loss:  0.2682668468178116\n",
      "loss:  0.26769844083722383\n",
      "loss:  0.26713228835009345\n",
      "loss:  0.266568362961829\n",
      "loss:  0.26600663823653437\n",
      "loss:  0.2654470868270784\n",
      "loss:  0.2648896794190921\n",
      "loss:  0.26433438352405436\n",
      "loss:  0.2637811621689304\n",
      "loss:  0.26322997253124003\n",
      "loss:  0.26268076455958217\n",
      "loss:  0.26213347960640915\n",
      "loss:  0.261588049095216\n",
      "loss:  0.2610443932677813\n",
      "loss:  0.26050242013789354\n",
      "loss:  0.2599620249599384\n",
      "loss:  0.25942309086642584\n",
      "loss:  0.25888549190923443\n",
      "loss:  0.2583491005674787\n",
      "loss:  0.25781380262189557\n",
      "loss:  0.2572795222585524\n",
      "loss:  0.256746257452375\n",
      "loss:  0.2562141178454996\n",
      "loss:  0.2556833454682348\n",
      "loss:  0.2551542925301398\n",
      "loss:  0.2546273460428603\n",
      "loss:  0.2541028266145879\n",
      "loss:  0.2535809155265757\n",
      "loss:  0.2530616459055774\n",
      "loss:  0.25254494674204975\n",
      "loss:  0.25203070136685407\n",
      "loss:  0.2515187911248832\n",
      "loss:  0.25100911700822337\n",
      "loss:  0.25050160521764275\n",
      "loss:  0.2499962047527713\n",
      "loss:  0.2494928824771912\n",
      "loss:  0.24899161822349655\n",
      "loss:  0.248492400750354\n",
      "loss:  0.2479952245883545\n",
      "loss:  0.24750008758466407\n",
      "loss:  0.24700698896501808\n",
      "loss:  0.24651592780644174\n",
      "loss:  0.24602690187972717\n",
      "loss:  0.24553990685153024\n",
      "loss:  0.2450549358337714\n",
      "loss:  0.2445719792463573\n",
      "loss:  0.24409102493442209\n",
      "loss:  0.2436120584654072\n",
      "loss:  0.24313506352915232\n",
      "loss:  0.24266002237382686\n",
      "loss:  0.2421869162276057\n",
      "loss:  0.2417157256743276\n",
      "loss:  0.2412464309676059\n",
      "loss:  0.2407790122799674\n",
      "loss:  0.2403134498912678\n",
      "loss:  0.23984972432449203\n",
      "loss:  0.23938781643862914\n",
      "loss:  0.23892770748792017\n",
      "loss:  0.2384693791557347\n",
      "loss:  0.23801281356991227\n",
      "loss:  0.23755799330501953\n",
      "loss:  0.237104901375654\n",
      "loss:  0.23665352122388764\n",
      "loss:  0.2362038367030669\n",
      "loss:  0.2357558320595694\n",
      "loss:  0.23530949191357706\n",
      "loss:  0.23486480123963574\n",
      "loss:  0.23442174534742607\n",
      "loss:  0.23398030986308804\n",
      "loss:  0.2335404807112263\n",
      "loss:  0.2331022440977183\n",
      "loss:  0.2326655864934328\n",
      "loss:  0.23223049461873943\n",
      "loss:  0.2317969554290224\n",
      "loss:  0.2313649561010599\n",
      "loss:  0.2309344840204541\n",
      "loss:  0.23050552677004207\n",
      "loss:  0.23007807211951584\n",
      "loss:  0.2296521080162669\n",
      "loss:  0.22922762257773888\n",
      "loss:  0.22880460408541242\n",
      "loss:  0.22838304098072185\n",
      "loss:  0.2279629218632573\n",
      "loss:  0.22754423549159578\n",
      "loss:  0.22712697078717753\n",
      "loss:  0.226711116841779\n",
      "loss:  0.22629666292906392\n",
      "loss:  0.22588359852077272\n",
      "loss:  0.22547191330817026\n",
      "loss:  0.2250615972292039\n",
      "loss:  0.22465264050186654\n",
      "loss:  0.22424503366401155\n",
      "loss:  0.22383876761953073\n",
      "loss:  0.22343383369062608\n",
      "loss:  0.22303022367507155\n",
      "loss:  0.22262792990694674\n",
      "loss:  0.22222694531827694\n",
      "loss:  0.22182726349834617\n",
      "loss:  0.22142887874625491\n",
      "loss:  0.22103178611183125\n",
      "loss:  0.22063598141917407\n",
      "loss:  0.2202414612674829\n",
      "loss:  0.21984822300413842\n",
      "loss:  0.2194562646668712\n",
      "loss:  0.2190655848937685\n",
      "loss:  0.21867618280307421\n",
      "loss:  0.21828805784780436\n",
      "loss:  0.21790120965312354\n",
      "loss:  0.2175156378468028\n",
      "loss:  0.21713134189403752\n",
      "loss:  0.21674832094753768\n",
      "loss:  0.21636657372199902\n",
      "loss:  0.2159860983993481\n",
      "loss:  0.21560689256761917\n",
      "loss:  0.21522895319316565\n",
      "loss:  0.21485227662297324\n",
      "loss:  0.2144768586119736\n",
      "loss:  0.2141026943692139\n",
      "loss:  0.21372977861664194\n",
      "loss:  0.2133581056546835\n",
      "loss:  0.2129876694298746\n",
      "loss:  0.21261846360083075\n",
      "loss:  0.21225048159992368\n",
      "loss:  0.21188371668919764\n",
      "loss:  0.21151816200959705\n",
      "loss:  0.21115381062344515\n",
      "loss:  0.21079065555039822\n",
      "loss:  0.21042868979728394\n",
      "loss:  0.21006790638260356\n",
      "loss:  0.20970829835623472\n",
      "loss:  0.20934985881515383\n",
      "loss:  0.20899258091566145\n",
      "loss:  0.20863645788277574\n",
      "loss:  0.208281483017255\n",
      "loss:  0.20792764970061514\n",
      "loss:  0.20757495139858084\n",
      "loss:  0.20722338166316076\n",
      "loss:  0.20687293413364652\n",
      "loss:  0.2065236025366919\n",
      "loss:  0.20617538068562485\n",
      "loss:  0.20582826247914207\n",
      "loss:  0.2054822418994404\n",
      "loss:  0.20513731300990345\n",
      "loss:  0.2047934699523776\n",
      "loss:  0.20445070694411657\n",
      "loss:  0.20410901827443864\n",
      "loss:  0.20376839830109417\n",
      "loss:  0.2034288414464763\n",
      "loss:  0.20309034219360111\n",
      "loss:  0.20275289508194289\n",
      "loss:  0.20241649470318737\n",
      "loss:  0.20208113569686229\n",
      "loss:  0.20174681274593112\n",
      "loss:  0.2014135205723214\n",
      "loss:  0.2010812539325043\n",
      "loss:  0.2007500076130402\n",
      "loss:  0.2004197764261763\n",
      "loss:  0.20009055520552854\n",
      "loss:  0.19976233880181557\n",
      "loss:  0.19943512207870417\n",
      "loss:  0.19910889990874503\n",
      "loss:  0.19878366716946808\n",
      "loss:  0.1984594187396035\n",
      "loss:  0.1981361494954838\n",
      "loss:  0.1978138543076325\n",
      "loss:  0.1974925280375783\n",
      "loss:  0.19717216553497893\n",
      "loss:  0.19685276163501383\n",
      "loss:  0.19653431115624337\n",
      "loss:  0.19621680889893298\n",
      "loss:  0.19590024964399383\n",
      "loss:  0.1955846281527318\n",
      "loss:  0.1952699391674827\n",
      "loss:  0.19495617741349866\n",
      "loss:  0.19464333760218763\n",
      "loss:  0.1943314144361655\n",
      "loss:  0.19402040261633924\n",
      "loss:  0.19371029685148208\n",
      "loss:  0.1934010918706694\n",
      "loss:  0.19309278243899983\n",
      "loss:  0.1927853633769588\n",
      "loss:  0.19247882958378246\n",
      "loss:  0.19217317606493559\n",
      "loss:  0.19186839796377714\n",
      "loss:  0.19156449059701305\n",
      "loss:  0.191261449493369\n",
      "loss:  0.19095927043421276\n",
      "loss:  0.19065794949452317\n",
      "loss:  0.19035748308195383\n",
      "loss:  0.1900578679711042\n",
      "loss:  0.18975910132993812\n",
      "loss:  0.1894611807348587\n",
      "loss:  0.1891641041713474\n",
      "loss:  0.18886787001749572\n",
      "loss:  0.18857247700900898\n",
      "loss:  0.18827792418556608\n",
      "loss:  0.18798421082030636\n",
      "loss:  0.1876913363360137\n",
      "loss:  0.18739930021294604\n",
      "loss:  0.18710810189454993\n",
      "loss:  0.18681774069735166\n",
      "loss:  0.18652821573102651\n",
      "loss:  0.18623952583332307\n",
      "loss:  0.18595166952290743\n",
      "loss:  0.18566464497130503\n",
      "loss:  0.1853784499932256\n",
      "loss:  0.18509308205331207\n",
      "loss:  0.1848085382861771\n",
      "loss:  0.18452481552629654\n",
      "loss:  0.1842419103443024\n",
      "loss:  0.18395981908654355\n",
      "loss:  0.18367853791541652\n",
      "loss:  0.1833980628485669\n",
      "loss:  0.18311838979576883\n",
      "loss:  0.1828395145926952\n",
      "loss:  0.1825614330314433\n",
      "loss:  0.1822841408878612\n",
      "loss:  0.18200763394584293\n",
      "loss:  0.18173190801914724\n",
      "loss:  0.1814569589709578\n",
      "loss:  0.18118278273165686\n",
      "loss:  0.18090937531506895\n",
      "loss:  0.18063673283335913\n",
      "loss:  0.18036485151063364\n",
      "loss:  0.18009372769523768\n",
      "loss:  0.17982335787055923\n",
      "loss:  0.17955373866403626\n",
      "loss:  0.1792848668541117\n",
      "loss:  0.1790167393746125\n",
      "loss:  0.17874935331624978\n",
      "loss:  0.17848270592476292\n",
      "loss:  0.1782167945954648\n",
      "loss:  0.17795161686396832\n",
      "loss:  0.17768717039303802\n",
      "loss:  0.1774234529558451\n",
      "loss:  0.17716046241585795\n",
      "loss:  0.17689819670409415\n",
      "loss:  0.17663665379437657\n",
      "loss:  0.17637583167761134\n",
      "loss:  0.17611572833592806\n",
      "loss:  0.17585634171773323\n",
      "loss:  0.17559766971451865\n",
      "loss:  0.17533971014016278\n",
      "loss:  0.1750824607132758\n",
      "loss:  0.17482591904290573\n",
      "loss:  0.17457008261767312\n",
      "loss:  0.1743149487982238\n",
      "loss:  0.1740605148126584\n",
      "loss:  0.1738067777544473\n",
      "loss:  0.17355373458231485\n",
      "loss:  0.17330138212139476\n",
      "loss:  0.1730497170650779\n",
      "loss:  0.1727987359769033\n",
      "loss:  0.17254843529190342\n",
      "loss:  0.17229881131695346\n",
      "loss:  0.1720498602295434\n",
      "loss:  0.17180157807467128\n",
      "loss:  0.17155396075944487\n",
      "loss:  0.1713070040450145\n",
      "loss:  0.17106070353559516\n",
      "loss:  0.1708150546642145\n",
      "loss:  0.17057005267482103\n",
      "loss:  0.1703256926004773\n",
      "loss:  0.1700819692371775\n",
      "loss:  0.1698388771129489\n",
      "loss:  0.16959641045178064\n",
      "loss:  0.16935456313199918\n",
      "loss:  0.16911332863873196\n",
      "loss:  0.16887270001031623\n",
      "loss:  0.16863266977867947\n",
      "loss:  0.16839322990424466\n",
      "loss:  0.1681543717066727\n",
      "loss:  0.16791608579386427\n",
      "loss:  0.16767836199333425\n",
      "loss:  0.16744118929256044\n",
      "loss:  0.16720455579835372\n",
      "loss:  0.166968448729746\n",
      "loss:  0.16673285446467742\n",
      "loss:  0.16649775866723526\n",
      "loss:  0.166263146528652\n",
      "loss:  0.16602900315941654\n",
      "loss:  0.1657953141681765\n",
      "loss:  0.16556206644989707\n",
      "loss:  0.16532924917436675\n",
      "loss:  0.16509685491097997\n",
      "loss:  0.16486488074747563\n",
      "loss:  0.16463332917329102\n",
      "loss:  0.16440220843499184\n",
      "loss:  0.16417153207872515\n",
      "loss:  0.16394131751183813\n",
      "loss:  0.16371158363931304\n",
      "loss:  0.16348234789159188\n",
      "loss:  0.16325362314362704\n",
      "loss:  0.16302541503608453\n",
      "loss:  0.1627977200430017\n",
      "loss:  0.1625705243783946\n",
      "loss:  0.16234380362889275\n",
      "loss:  0.16211752293619552\n",
      "loss:  0.16189163766501152\n",
      "loss:  0.16166609476634355\n",
      "loss:  0.16144083545596696\n",
      "loss:  0.16121580031139174\n",
      "loss:  0.16099093825119223\n",
      "loss:  0.16076622058263224\n",
      "loss:  0.1605416594609152\n",
      "loss:  0.16031732589580625\n",
      "loss:  0.16009335704258099\n",
      "loss:  0.1598699411596326\n",
      "loss:  0.15964727791465305\n",
      "loss:  0.15942552928294967\n",
      "loss:  0.15920478593107368\n",
      "loss:  0.1589850633494113\n",
      "loss:  0.15876632158688606\n",
      "loss:  0.1585484920961575\n",
      "loss:  0.15833149923256196\n",
      "loss:  0.15811527264646982\n",
      "loss:  0.15789975227163372\n",
      "loss:  0.15768488896329255\n",
      "loss:  0.15747064318116394\n",
      "loss:  0.15725698310330016\n",
      "loss:  0.15704388281618556\n",
      "loss:  0.15683132081093076\n",
      "loss:  0.15661927881991078\n",
      "loss:  0.15640774095279134\n",
      "loss:  0.1561966930718622\n",
      "loss:  0.1559861223503832\n",
      "loss:  0.15577601696961432\n",
      "loss:  0.15556636592311657\n",
      "loss:  0.1553571589095192\n",
      "loss:  0.15514838630640443\n",
      "loss:  0.1549400392284177\n",
      "loss:  0.15473210968191184\n",
      "loss:  0.15452459083512188\n",
      "loss:  0.1543174774249115\n",
      "loss:  0.15411076631456327\n",
      "loss:  0.1539044571975041\n",
      "loss:  0.15369855340432453\n",
      "loss:  0.15349306271457014\n",
      "loss:  0.15328799800790885\n",
      "loss:  0.15308337753334747\n",
      "loss:  0.15287922456500397\n",
      "loss:  0.15267556628684964\n",
      "loss:  0.1524724319195732\n",
      "loss:  0.15226985033312304\n",
      "loss:  0.15206784758629868\n",
      "loss:  0.1518664448966314\n",
      "loss:  0.15166565742426463\n",
      "loss:  0.15146549400404105\n",
      "loss:  0.15126595769865672\n",
      "loss:  0.15106704687937958\n",
      "loss:  0.1508687565072872\n",
      "loss:  0.15067107935545054\n",
      "loss:  0.15047400702059183\n",
      "loss:  0.15027753067161237\n",
      "loss:  0.15008164154855252\n",
      "loss:  0.14988633125819337\n",
      "loss:  0.1496915919206893\n",
      "loss:  0.14949741621635626\n",
      "loss:  0.14930379737139515\n",
      "loss:  0.14911072911033355\n",
      "loss:  0.14891820559382396\n",
      "loss:  0.14872622135333602\n",
      "loss:  0.14853477122949815\n",
      "loss:  0.1483438503174653\n",
      "loss:  0.14815345392064488\n",
      "loss:  0.1479635775129168\n",
      "loss:  0.14777421670866003\n",
      "loss:  0.14758536723968452\n",
      "loss:  0.14739702493794235\n",
      "loss:  0.14720918572296968\n",
      "loss:  0.14702184559301235\n",
      "loss:  0.14683500061901572\n",
      "loss:  0.1466486469407\n",
      "loss:  0.1464627807641049\n",
      "loss:  0.14627739836014142\n",
      "loss:  0.14609249606379496\n",
      "loss:  0.1459080702736259\n",
      "loss:  0.14572411745143277\n",
      "loss:  0.14554063412194793\n",
      "loss:  0.14535761687238913\n",
      "loss:  0.14517506235192557\n",
      "loss:  0.1449929672709436\n",
      "loss:  0.14481132840018335\n",
      "loss:  0.14463014256972603\n",
      "loss:  0.1444494066678466\n",
      "loss:  0.1442691176398032\n",
      "loss:  0.14408927248654899\n",
      "loss:  0.1439098682634822\n",
      "loss:  0.14373090207916683\n",
      "loss:  0.1435523710941909\n",
      "loss:  0.1433742725200827\n",
      "loss:  0.14319660361842584\n",
      "loss:  0.14301936170014173\n",
      "loss:  0.14284254412502267\n",
      "loss:  0.14266614830152607\n",
      "loss:  0.1424901716868842\n",
      "loss:  0.14231461178755236\n",
      "loss:  0.14213946616001452\n",
      "loss:  0.14196473241201626\n",
      "loss:  0.14179040820417219\n",
      "loss:  0.14161649125202966\n",
      "loss:  0.14144297932857547\n",
      "loss:  0.1412698702671272\n",
      "loss:  0.1410971619646672\n",
      "loss:  0.140924852385514\n",
      "loss:  0.14075293956531595\n",
      "loss:  0.14058142161527162\n",
      "loss:  0.14041029672647645\n",
      "loss:  0.14023956317423641\n",
      "loss:  0.14006921932230085\n",
      "loss:  0.139899263626695\n",
      "loss:  0.1397296946390734\n",
      "loss:  0.13956051100933858\n",
      "loss:  0.13939171148733148\n",
      "loss:  0.13922329492332713\n",
      "loss:  0.13905526026724463\n",
      "loss:  0.1388876065662916\n",
      "loss:  0.13872033296098166\n",
      "loss:  0.1385534386794811\n",
      "loss:  0.13838692303017514\n",
      "loss:  0.13822078539265048\n",
      "loss:  0.13805502520716112\n",
      "loss:  0.1378896419628205\n",
      "loss:  0.13772463518484732\n",
      "loss:  0.13756000442115834\n",
      "loss:  0.13739574922874953\n",
      "loss:  0.13723186916020563\n",
      "loss:  0.13706836375079687\n",
      "loss:  0.13690523250647985\n",
      "loss:  0.13674247489311253\n",
      "loss:  0.13658009032716242\n",
      "loss:  0.1364180781680325\n",
      "loss:  0.13625643771214188\n",
      "loss:  0.13609516818872255\n",
      "loss:  0.13593426875731532\n",
      "loss:  0.13577373850681618\n",
      "loss:  0.13561357645593744\n",
      "loss:  0.13545378155485535\n",
      "loss:  0.1352943526878706\n",
      "loss:  0.13513528867685454\n",
      "loss:  0.13497658828524725\n",
      "loss:  0.13481825022245492\n",
      "loss:  0.1346602731484912\n",
      "loss:  0.1345026556786422\n",
      "loss:  0.1343453963881286\n",
      "loss:  0.13418849381663592\n",
      "loss:  0.13403194647259747\n",
      "loss:  0.133875752837321\n",
      "loss:  0.13371991136876418\n",
      "loss:  0.13356442050510192\n",
      "loss:  0.13340927866798746\n",
      "loss:  0.13325448426562664\n",
      "loss:  0.13310003569557297\n",
      "loss:  0.1329459313473885\n",
      "loss:  0.1327921696051844\n",
      "loss:  0.1326387488500297\n",
      "loss:  0.13248566746238744\n",
      "loss:  0.13233292382452036\n",
      "loss:  0.13218051632299446\n",
      "loss:  0.13202844335134303\n",
      "loss:  0.13187670331287732\n",
      "loss:  0.13172529462374585\n",
      "loss:  0.13157421571632552\n",
      "loss:  0.13142346504290447\n",
      "loss:  0.13127304107972018\n",
      "loss:  0.13112294233143912\n",
      "loss:  0.13097316733597464\n",
      "loss:  0.13082371466968984\n",
      "loss:  0.13067458295293446\n",
      "loss:  0.1305257708558359\n",
      "loss:  0.13037727710421423\n",
      "loss:  0.13022910048551697\n",
      "loss:  0.13008123985456488\n",
      "loss:  0.1299336941388699\n",
      "loss:  0.1297864623433269\n",
      "loss:  0.1296395435539412\n",
      "loss:  0.12949293694035008\n",
      "loss:  0.12934664175680977\n",
      "loss:  0.12920065734146718\n",
      "loss:  0.12905498311362293\n",
      "loss:  0.12890961856895383\n",
      "loss:  0.12876456327256808\n",
      "loss:  0.1286198168500437\n",
      "loss:  0.1284753789766234\n",
      "loss:  0.1283312493648536\n",
      "loss:  0.12818742775116962\n",
      "loss:  0.12804391388183545\n",
      "loss:  0.1279007074988396\n",
      "loss:  0.12775780832631492\n",
      "loss:  0.127615216057994\n",
      "loss:  0.12747293034613372\n",
      "loss:  0.12733095079232548\n",
      "loss:  0.12718927694038284\n",
      "loss:  0.1270479082714142\n",
      "loss:  0.12690684420112547\n",
      "loss:  0.1267660840791505\n",
      "loss:  0.12662562719024661\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[179], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m twoLayerNet()\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train\u001b[39m.\u001b[39;49mT,y_train)\n",
      "Cell \u001b[1;32mIn[178], line 62\u001b[0m, in \u001b[0;36mtwoLayerNet.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(a2, y, m))\n\u001b[0;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward(X, y, \u001b[39m0.01\u001b[39m, z1, a1, z2, a2)\n\u001b[1;32m---> 62\u001b[0m     z1, a1, z2, a2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(X)\n\u001b[0;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\n",
      "Cell \u001b[1;32mIn[178], line 13\u001b[0m, in \u001b[0;36mtwoLayerNet.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     11\u001b[0m W1, W2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m b1, b2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mb2\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m z1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39mij,jk->ik\u001b[39;49m\u001b[39m\"\u001b[39;49m,W1\u001b[39m.\u001b[39;49mT,X) \u001b[39m+\u001b[39m b1\n\u001b[0;32m     14\u001b[0m a1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(z1)\n\u001b[0;32m     15\u001b[0m z2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mij,jk->k\u001b[39m\u001b[39m\"\u001b[39m,W2\u001b[39m.\u001b[39mT,a1) \u001b[39m+\u001b[39m b2\n",
      "File \u001b[1;32mc:\\Users\\potat\\miniconda3\\envs\\ds\\lib\\site-packages\\numpy\\core\\einsumfunc.py:1371\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[39mif\u001b[39;00m specified_out:\n\u001b[0;32m   1370\u001b[0m         kwargs[\u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m out\n\u001b[1;32m-> 1371\u001b[0m     \u001b[39mreturn\u001b[39;00m c_einsum(\u001b[39m*\u001b[39moperands, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1373\u001b[0m \u001b[39m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[39m# repeat default values here\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m valid_einsum_kwargs \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morder\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcasting\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = twoLayerNet()\n",
    "model.fit(X_train.T,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(X_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9988662131519275\n"
     ]
    }
   ],
   "source": [
    "print(\"Score: \",np.where(pred_y == y_test,1,0).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[821,   0],\n",
       "       [  2, 941]], dtype=int64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open new file save the model and weighs using pickle\n",
    "import pickle\n",
    "pickle.dump(model, open('model.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open pickle model file\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4888401440823957"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = np.random.rand(3)\n",
    "test2 = np.random.randint(0, 2, 1)\n",
    "model.loss(test1,test2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24790999, 0.98393852, 0.66169522])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62245933, 0.62245933, 0.62245933, 0.62245933, 0.62245933],\n",
       "       [0.62245933, 0.62245933, 0.62245933, 0.62245933, 0.62245933]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = np.random.rand(2, 4)\n",
    "test2 = np.random.randint(0, 1, (2, 4))\n",
    "model.sigmoid(np.array([[0.5, 0.5, 0.5, 0.5, 0.5],[0.5, 0.5, 0.5, 0.5, 0.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.random.rand(2, 4)\n",
    "test2 = np.random.randint(0, 1, (2, 4))\n",
    "\n",
    "#test2 = np.random.randn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'conctest1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (np\u001b[39m.\u001b[39;49mconctest1 \u001b[39m+\u001b[39m test2)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\potat\\miniconda3\\envs\\ds\\lib\\site-packages\\numpy\\__init__.py:328\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mRemoved in NumPy 1.25.0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTester was removed in NumPy 1.25.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 328\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    329\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'conctest1'"
     ]
    }
   ],
   "source": [
    "(np.conctest1 + test2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
